{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a90f8caa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exercise 1: Performing multiple independent MD simulations in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12dd512",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Running simulations in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b78ea",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this exercise, the goal is to run 4 indepedent standard NPT MD simulations of the NaCl system in parallel, each for 500 ps. This can be achieved by using the `-multidir` flag in the GROMACS `mdrun` command, which runs multiple simulations simultaneously in different folders. This exercise should be performed in an interactive session on Bridges-2, with 32 cores requested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce746c64",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Delete folders sim_*, if there is any. This is not necessary but convenient for rerunning the notebook. \n",
    "if ls -d sim_* >/dev/null 2>&1; then\n",
    "  rm -rf sim_*\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6340acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To get started, we execute the following commands to set up the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5fe2f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "module load gromacs/2020.2-cpu openmpi/4.0.5-gcc10.2.0\n",
    "\n",
    "n=4  # number of simulations\n",
    "for i in {1..4}\n",
    "do\n",
    "    mkdir sim_${i} && cd sim_${i} && cp ../../Inputs/NaCl/* .   # create a folder and copy over gro, top and mdp files\n",
    "    mpirun -np 1 gmx_mpi grompp -f MD-NPT.mdp -c NaCl.gro -p NaCl.top -o NaCl_md.tpr -maxwarn 1 && cd ../  # Generate the tpr\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a565ad",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Step 1**: Create 4 folders: `sim_1`, `sim_2`, `sim_3`, and `sim_4`.\n",
    "- **Step 2**: For each folder, copy over the input files, including `NaCl.gro`, `NaCl.top`, and `NPT-MD.mdp`.\n",
    "- **Step 3**: In each folder, run the GROMACS `grompp` command in the folder to generate the input `tpr` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc657b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After setting up the simulation inputs, we can run the GROMACS `mdrun` command. Note that the `mdrun` command should be executed one level above the `sim_*` directories (which is the working directory for each cell in this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd41df3",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "module load gromacs/2020.2-cpu\n",
    "module load openmpi/4.0.5-gcc10.2.0\n",
    "\n",
    "mpirun -np 8 gmx_mpi mdrun -s NaCl_md.tpr -deffnm output -multidir sim_1 sim_2 sim_3 sim_4 -ntomp 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2224df8b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- `-np` specifies the number of MPI processes to run the 4 simulations and has to be a multiple of 4.\n",
    "- `-ntomp` specifies the number of OpenMP threads per MPI process. Generally, each OpenMP thread uses a CPU core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a1a10",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, we ran the simulations with 8 MPI processes (`-np 8`), each with 2 OpenMP threads per MPI process (`-ntomp 2`). This means: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1299c5",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Each of the 4 simulations was performed with 2 MPI processes, and the workload (e.g. the calculation of interactions) of running a simulation was further divided by 2 threads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830be92a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- A total of 16 CPU cores were used to run the 4 simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75319e34",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- That is, 16 CPU cores were idle during the simulation, so we could have only requested 16 cores when launching the interactive session by using `--ntasks-per-node=16`. (Here we requested 32 cores just for the scaling test in the next section.) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273a2cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With this setup, it took about 45 seconds to finish all 4 simulations. As can be checked, now we have simulation outputs in each `sim_*` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9972eb34",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. CPU scaling: Now, it's your turn! üë®‚Äçüíª"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444c20d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the example above, we used 8 MPI processes (denoted as `np`), with 2 OpenMP threads per MPI process (denoted as `ntomp`), to run 4 simulations in parrallel. To determine a more efficient/cost-effective way to allocate comptuational resources for running MD simulations, it is common to perform a scaling test. This involves timing several short simulations with varying values of `np` or `ntomp`. By plotting the performance, such as ns/day (as demonstrated in the bottom of the log file), against `np` or `ntomp`, we can determine the cutoff point where the performance becomes sublinear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b029b8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As an exercise, try running 4 simulations of NaCl in parallel using 4 different values of `ntomp`, including 1, 2, 4, and 8, while keeping `np` fixed at 4. Then, plot the performance as a function of the **the number of cores** and answer the following question: **What is the optimal number of OpenMP threads per MPI process required to run 4 standard MD simulations for the NaCl system with 4 MPI processes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7038b1c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To extract the performance of a simulation from its log file, you may find the following function useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58f1a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_performance(log):\n",
    "    with open(log, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for l in lines:\n",
    "        if 'Performance' in l:\n",
    "            return float(l.split()[1])\n",
    "\n",
    "    print('Simulation not finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18dc378",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "%%bash\n",
    "module load gromacs/2020.2-cpu openmpi/4.0.5-gcc10.2.0\n",
    "\n",
    "ntomp=(1 2 4 8)  # 8 is the maximum since we have 128 cores and we fix np at 4\n",
    "for i in \"${ntomp[@]}\"\n",
    "do\n",
    "    mpirun -np 4 gmx_mpi mdrun -s NaCl_md -deffnm output_$i -multidir sim_1 sim_2 sim_3 sim_4 -ntomp $i\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1955cb58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As shown above, we tried running the simulations using 1, 2, 4, and 8 OpenMP threads per MPI process. Ideally, is should take about 2 minutes to complete the execution of the cell above. From the bottom of each log file, we can get the performance in ns/day for each case, which we can plot below. (Note that the result may vary for different runs, especially for a such small system. However, for larger systems (which are more common), there should be a kink point beyond which the performance becomes sublinear.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8129d075",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "# Just some settings for plotting\n",
    "rc('font', **{\n",
    "    'family': 'sans-serif',\n",
    "    'sans-serif': ['DejaVu Sans'],\n",
    "    'size': 10,\n",
    "})\n",
    "# Set the font used for MathJax - more on thiprint(images)\n",
    "rc('mathtext', **{'default': 'regular'})\n",
    "plt.rc('font', family='serif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d2bf03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "n_mpi = 4\n",
    "ntomp = np.array([1, 2, 4, 8])\n",
    "p_avg, p_std = [], []\n",
    "for i in ntomp:\n",
    "    p_list = []\n",
    "    for j in range(1, 5):  # from sim_1 to sim_4\n",
    "        p_list.append(get_performance(f'sim_{j}/output_{i}.log'))\n",
    "    p_avg.append(np.mean(p_list))\n",
    "    p_std.append(np.std(p_list))\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(ntomp * n_mpi, p_avg, yerr=p_std, marker='o', capsize=2)\n",
    "plt.xlabel('Number of CPU cores')\n",
    "plt.ylabel('Performance (ns/day)')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e022eb7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that\n",
    "- We could have requested 128 cores to test a wider range of the number of cores. Here we requested only 32 cores considering available computing resources for the workshop.\n",
    "- The optimal number of `np`/`ntomp` for running MD simulations may vary depending on the system being studied and the simulation method used. Therefore, it is generally good practice to conduct a scaling test when kicking off a new proejct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee706970",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daf51b1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- GROMACS simulations can be performed in parallel using the `-multidir` option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f175678",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Run a scaling test for your systems/simulation methods if you plan to spend a lot ouf computational resources on them!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
